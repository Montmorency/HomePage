\chapter{Introduction}
\section{Electronic Structure for Materials Science}
In 1987 Henry Ehrenreich published an article in Science called
"Electronic Theory for Materials Science". The article begins
with what might now be considered a bold statement:

\begin{quote}
Theoretical Materials Science remains to be invented as a discipline, and for good reason.
\end{quote}

The reason he supplies is that materials processing, historically, was a craft. 
Roman engineers could manufacture concrete which was in some ways superior to that
made today without any specific knowledge of the microstructure of the material.
A blacksmith learned that the hardness of swords and shields could be manipulated 
by the alloying of metals and precise control of heat. This expertise was not accumulated 
based on reasoning from first principles but based on experience, trial and error, 
and general trade craft.

A satisfactory atomic theory and the techniques to solve the equations which arise in that theory
have only arisen in the last 100 years. Over the past 40 years the possibility of constructing
accurate models of materials systems beginning from the arrangment of the constituent atoms has
driven a great deal of what is sometimes called first principles or {\it ab initio} modelling.

The extent to which theoretical materials science as a discipline has evolved is worth examining. 
It is important because the character of this evolution dictates which directions of research may prove most
immediately fruitful. It is also important because it allows researchers to identify the many avenues
where people are going backwards, obscuring knowledge, and in some sense unlearning or convoluting 
what is already understood. 

To chart this evolution we may begin from Ehrenreich's survey of the field from 30 years ago.

Ehrenreich identified four points that characterized the state of the field of material science at the time:
%
\begin{enumerate}[i)]
\item Theory must be closely linked to experiment.
\item Ab initio calculations without approximations are virtually impossible except in model systems.
\item Approximations are frequently based on prior experience rather than on theoretical justification.
\item The study of trends in the properties of homologous materials 
      is a crucial ingredient of a credible theoretical framework.
\end{enumerate}
%

It is worth while considering how the intervening 30 years may have adjusted some of 
these priorities and characterize the current state of the field.

\section{The Rise of the Computer}
The intervening thirty years has seen an enormous extension of 
computing capacity. 

Around the time Ehrenreich was writing an example of a 
state of the art computer would be the Connection Machine.
\footnote{One of the employees at Thinking Machines Corporation,
the company building the connection machine, was Richard Feynman.
For an interesting discussion of the work they
did at Thinking Machines and Feynman's time there see W. Daniel
Hillis' article in Physics Today 42, 2, 78 (1989).
There was a strong focus on design at the company. Some of that
work can be found at Tamiko Thiel's page}%; \url{http://www.tamikothiel.com/theory/cm_txts/di-frames.html}}

The CM-2 was launched in 1987 It was configured with up to 512 MB of RAM and 25 GB of RAID
hard disk space. It had 65K microprocessors (each with 600 bytes of memory). Support for
32 bit floating point operations meant just over 2048 processors in practice per machine.

By contrast a standard HPC machine accessible at the moment to a university (2017) 
is measured in nodes each of which 2x12 core Intel Broadwell processors.
Each node has 128 GB RAM and 120 GB static storage. An accessible 
cluster of these nodes could include up to 720 nodes. The processors themselves have 
up to 6,8 or 10 cores with clock speeds on the order of 3 GHz, and, relative to 1987,
abundant cache space of 10x 256 KiB L2 Cache, and 25 MiB of L3 cache. 
By comparison a clock speed of 12.5 MHz would be cutting edge in 1987. 
There could be on the order of 3 billion transistors in a single microprocessor now;
in 1987 that number would be closer to 100,000. 

The constant development of computing capacity is an important component of enabling
{\it ab initio} calculations. 

What are the scales involved in connecting atomic systems where quantum mechanics is important
to macroscopic systems? Chemists arrived at the mol as a convenient unit for describing
quantities of atoms which is on the order of $10^{23}$ atoms. This is clearly impractical.

More realisitically a suitably macroscopic region for human purposes might be on the 
order of 200 nm$^{3}$ of material. A sample of 
material of this size is of the same order as the wavelength of visible light and could be
resolved with an optical microscope. Thus a dynamical simulation of around 8 billion atoms
done using quantum mechanics could be considered a quantum mechanical simulation of
a macroscopic system. If computer technology continues developing at the same rate it has
over the previous 30 years than within 60 years, with no developements in the numerical
algorithms their efficiency etc., quantum mechanical simulations of systems of these 
size should be feasible. I am not so sure that the same changes in computing
power can be expected. Much like the leveling off of transport technology from
horse, to train, to plane. A flight from the UK to England may have taken 12 hours
50 years ago. Now it takes 7. The improvement is incremental 
%and unlikely to change
%significantly in the next 60 years (if anything it will become slower).

The development in computing hardware is in some ways the easy part to chart. 
Clock speeds, transistor density, and the proliferation of high 
performance computing clusters with fast interconnects have greatly extended 
the range of problems that can be treated with numerical simulations
simply by providing more horse power.

Along with advances in hardware there has been contemporaneous methodological developments.
Compiler optimization, development of modern programming languages, 
shared libraries, object orientation, and stable open source platforms for 
code development, testing, and sharing. The theoretical advance is more difficult
to quantify in what exactly it has enabled.

\section{The Rise of Technique}
Given a really good computer what would you like to do with it?

Write documents and send them to people? Create a system where the computers share files
with each other? Listen to music? Watch films? Learn Things? Compute things? The latter is in some
ways becoming a niche application of computers.

What sort of things should a computer compute? Well before things become too philosophical we
can answer this question definitively.
For our purposes we wish to calculate O. K. Andersen's wish list, we want to:
"... compute groundstate properties such as cohesive energies, interatomic forces, charge transfer, and magnetic moments,
and also excitation spectra described by the one-electron Green's function." \cite{anderson75} 
Essentially from our point of view computers exist to simulate the behaviour of atoms.

The theoretical engines driving progress in numerical simulations of materials systems are
largely based on density functional theory, pseudopotentials, and the various numerical algorithms
for computing the eigenstates of electrons in material systems, and solving the coupled systems
of linear equations which arise again and again when doing self-consistent 
electronic structure calculations.

Looking at the citation metrics, foundational papers in the subject
of DFT and methods number their citations in the tens of thousands, for the 
theoretical and algorithmic improvements improvements are sufficient testament 
to the reach and fundamental nature of {\it ab initio} calculations. 
Table~\ref{tab:foundation} lists some of the papers in the field which, as
their citation count suggests, have enabled subsequent research. These techniques fall into
three categories, they are either theoretical justification for a calculation scheme, the description of
a technique that allows for more expedient and accurate calculations to be performed, or a parameterization
of the the exchange correlation functional.

\begin{table}
\begin{tabular}{|l|l|l|l|}
\hline
Paper & Citations & Development & Category\\
\hline
Inhomogeneous Electron Gas \cite{hohenberg64} & 20,527 & Ground state energy of electron gas is universal functional of the density. & Theory \\
Self-Consistent Equations Including Exchange and Correlation Effects & 26,273 & Self-consistent set of equations for varying electron density. & Theory \\
Linear methods in band theory \cite{andersen75}& 4,822 & Describes LAPW and LMTO approach to electronic structure calculations. & Theory/Implementation \\
Norm-Conserving Pseudopotentials \cite{hamann79} & 2,156 & Nodeless eigenfunctions that match atomic eigenvalues.  & Pseudopotentials \\
Soft self-consistent pseudopotentials in a generalized eigenvalue formalism \cite{vanderbilt90} & 11,560 & Effective pseudopotentials for first row and transition-metal systems  & Pseudopotentials \\
Projector augmented-wave method \cite{blochl94paw} & 19,194 & Generalizes pseudopotential and LAPW methods; allows reconstruction of wavefunctions at nucleus.  & Pseudopotentials \\
Efficacious Form for Model Pseudopotentials \cite{kleinman82} & 3,709 & Reduces number of integrals which need to be calculated & Pseudopotentials \\
Special points for Brillouin-zone integrations \cite{monkhorst76} & 21,361 & Brillouin zone integration & Numerical Integration Technique \\
Ground State of the Electron Gas by a Stochastic Method \cite{ceperley80} & 8,855 & Calculation of the exchange-correlation energy of an electron gas. & Exchange Correlation \\
Self-interaction correction to density-functional approximations for many-electron systems \cite{perdew81} & 11,629 & Parameterization of Ceperley-Alder & Exchange Correlation \\
Accurate and simple analytic representation of the electron-gas correlation energy \cite{perdew91} & 12,296 & Parameterization of exchange correlation functional & Exchange Correlation \\
Atoms, molecules, solids, and surfaces: Applications of the generalized gradient approximation for exchange and correlation \cite{perdew92} & 11,418 & Generalized gradient expansion& Exchange Correlation \\
Electron correlation in semiconductors and insulators: Band gaps and quasiparticle energies \cite{hybertsen86} & 1,929 & Extension of electronic structure calculations to excited states& Theory/Implementation \\
Unified Approach for Molecular Dynamics and Density-Functional Theory \cite{car85} & 5,940 & Combined molecular dynamics and density functional theory & Theory/Implementation \\
High-precision sampling for Brillouin-zone integration in metals \cite{methfessel89} & 2,908 & Brillouin zone integration & Numerical Integration Technique \\
Improved tetrahedron method for Brillouin-zone integrations \cite{blochl94} &  2,986 & Brillouin zone integration & Numerical Integration Technique\\
A new mixing of Hartree–Fock and local density‐functional theories \cite{becke93} & 7,143 & Combining Hartree-Fock and DFT & Exchange Correlation \\
Generalized Gradient Approximation Made Simple \cite{perdew96} & 47,029 & GGA functionals improved.& Exchange Correlation \\
\end{tabular}
\caption{Citations are relevant up to Nov. 2017. Citations are according to the journals in which they appear, the actual number of
citations are much higher. These selections have been chosen as representative of the important theoretical and algorithmic developments
which have enabled subsequent research. In some cases there are a number of contemporary papers which
treat the same problems but failed to "catch on" or describe techniques which differ in an incremental way to the works cited here. 
\label{tab:foundation}}
\end{table}

This table is biased towards a "plane wave" conception of electronic structure. These methods are suitable for crystalline systems
with periodicity and can be pushed to describe more disordered systems with a few 100 atoms. 

To broadly categorize the workers we can say a material scientist 
is interested in crystals, alloys, ceramics, glasses, and possibly elastic materials. They
worry about energy differences on the order of 100 meV. Chemists are interested in molecules, tend to use localized basis sets and 
exploit Monte Carlo methods to obtain energy bounds on the order of a few meV. Physicists are interested in lasers and hubbard models and the 
general positions of resonances and asymptotes in very cold materials and wont be discussed further.

If we look at the work done based on parameterization of the exchange correlation functional we see the number of citations approaches 100,000. 
Naturally that figure includes significant double counting of citing work but we have also not included cross citations between journals in many of the 
figures quoted so the number is probably representative. This parameterization is based on what I consider a highly abstract question.
"What might happen if we start squeezing electrons into an imaginary box?".

This imaginary box has been pondered for close to a century now. The practical utility of these considerations is unreasonable. 
The outcome of these considerations have resulted in a family of unremarkable curves that describe some energy relationship between
the number density of electrons at a point in space and an energy. The practical consequence is this curve enables the
structural, electronic, and vibrational properties of an enormous class of materials to be calculated with highly creditable accuracy.

How creditable this approach is returns again to point three. The theoretical justifications for the success of the approach are
secondary to the fact that prior experience suggests these techniques work, and the rate of their acceptance and adoption
seems to have tracked their usage in a positive feedback loop. For instance an article from 12 years ago 

If you remain unconvinced of the accuracy of the method one can at least acknowledge the social benefit. 
The sheer number of hours people have been engaged in this harmless occupation, dreaming of 
electrons in an imaginary box, means they have a hobby that keeps them off the streets and out of trouble.


The computational price paid to obtain these quantities should be considered. If the amount of numerical
work required to compute the quantities is excessive it becomes difficult to discuss trends in homologous series of materials, perform
necessary thermal averages to describe distributions, and even in some cases reproduce exactly the calculation.

\section{Point 1: Decline of Experimental Overlap}
The cost of the ubiquity of access to {\it ab initio} data has had a significant impact on 
Ehrenreich's first point (i): i.e., experimental corroboration. Cursory surveys of the literature 
demonstrate the increasing frequency of appearance of publications which contain no 
original experimental work. 

This is partly down to the high level of specialization of contemporary 
researchers and research groups. Previously the distinction between
an experimental materials scientist and a theoretical one was not so dramatic. 
In addition many important theoretical developments came 
from the context of attempting to explain a new set of experimental data.

%This paragraph needs research:
%Bethe's calculation of the hyperfine? shift, the recursion work on the magnetic 
%moment of $FeAl_{3}$, I think Bardeen mentioned his work on superconductivity was
%initiated when they were trying, Shockley's work on the transistor. (A good source 
%for these sorts of anecdotal history can be found in Nobel prize banquet speeches.)

\section{Point 3: Justification by Utility}
The observation that materials processing has evolved in the manner of a craft appears to have
been imitated in the development of the theory of materials science. DFT is an excellent example of this.
If you look through a typical electronic structure code it is interesting to look at the routine
responsible for calculating the local exchange correlation potential. The input required is the 
electronic density at a point in space. This density is inserted as the soul argument in a rational polynomial
and a single number is returned. Yet in all manner of crystalline systems this functional enables the theoretical
prediction of very accurate ground state properties and structures. 

Another successful theoretical development is pseudopotentials. Focusing on the valence electrons 
has worked for chemistry.

\section{Bayesian Materials Science}
Ehrenreich's four points help clarify a desirable objective for materials modelling:
a minimal theoretical model informed by {\it ab initio} calculations that accounts for 
experimental data, and {\emph predicts} quantitatively trends in material properties. 

In these notes the possibility of exploiting the invariance theorem 
and recursion techniques is assessed as the optimal framework
for achieving this. A metric for the quality of the approach can be defined using 
Bayesian probability theory. The Bayesian framework lets us define a metric 
that increases as the number of free parameters in the model is reduced,
and the reproduction of target experimental data is increased, with a bound on optimal predictions.

The posterior on the parameters of the model can be assessed:
%
\begin{equation}
\label{eq:bayes}
P(\mathbf{w}|D, \mathcal{H}_{i}) = \frac{P(D|\mathbf{w}, \mathcal{H}_{i})P(\mathbf{w}|\mathcal{H})}{P(D|\mathcal{H}_{i})},
\end{equation}
%
where D, the target data, can constitute a combination of {\it ab initio} and experimental data. Eq.~\ref{eq:bayes}
McKay summarises as:
\begin{equation}
{\rm Posterior} = \frac{{\rm Likelihood} \times {\rm Prior}}{{\rm Evidence}}
\end{equation}
%
\begin{equation}
\label{eq:bayesH}
P(\mathcal{H}_{i}|D) \propto P(D|\mathcal{H}_{i})P(\mathcal{H}_{i})
\end{equation}
%
\begin{equation}
\label{eq:bayesH}
P(D|\mathcal{H}_{i}) = P(D|\mathbf{w}, \mathcal{H}_{i})P(\mathbf{w}|\mathcal{H}_{i})d\mathbf{w}
\end{equation}
%
Models (hypotheses) of material systems can then be ranked according to Eq.~\ref{eq:bayesH}.

