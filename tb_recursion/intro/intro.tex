\chapter{Introduction}
\section{Electronic Structure for Materials Science}
In 1987 Henry Ehrenreich published an article in Science called
``Electronic Theory for Materials Science". The article begins
with what might now be considered a bold statement:

\begin{quote}
Theoretical Materials Science remains to be invented as a discipline, and for good reason.
\end{quote}

The reason he supplies is that materials processing, historically, was a craft. 

Roman concrete presents a suitable example of craftsmanship.
Roman engineers learned to manufacture concrete from a 
particular mixture of volcanic sands called pozzolana,
taking its name from Pozzuoli near Naples, mixed with quicklime, an aggregate, and seawater.
This concrete was of striking quality. The pantheon today remains standing 
built with the same concrete poured centuries before.
This concrete was particularly resistant to degradation in the presence of moisture.
\footnote{For a description of roman concrete and its use at the port at Pouzol see: Mariano Vasi, 
Itineraire Instructif de Rome à Naples, ou, Description Generale des Monumens Anciens et Modernes, 
Rome: [Mariano Vasi]. 1813, de Beer Itb 1815 V.}. 
This concrete was manufactured without any specific knowledge of microstructure and 
chemistry. How it even came to be discovered is somewhat mysterious.

Equally crafty we have the blacksmiths who over the centuries learned to manipulate the 
hardness of swords and shields by the alloying of metals and precise control of heat. 
Again the theoretical model they worked with lacked the modern conception of 
atomic composition and specific microstructure.

In each case the material practitioner's expertise was not accumulated based on reasoning 
from first principles but based on experience, trial and error, and craft.

A comprehensive atomic theory describing the way atoms bind, the forces acting on them, 
and the techniques to solve the equations which arise, has only arisen in the last 100 years. 
Over the past 40 years the combination of the theory and access to computers 
means the possibility of constructing accurate models of materials 
systems beginning from the arrangment of the constituent atoms has
progressed giving rise to the large field of "first principles" or {\it ab initio}
\footnote{ab initio (latin): from the beginning.} modelling.

The extent to which theoretical materials science as a discipline has evolved 
over this period of time is worth examining. 
It is important because the character of this evolution dictates which 
directions of research may prove most immediately fruitful. It is also important 
because it allows researchers to identify the avenues
of research that are inefficient, that are potentially obscuring knowledge, 
and convoluting what is already understood. 

To chart this evolution we may begin from Ehrenreich's survey of the field from 30 years ago.

Ehrenreich identified four points that either characterized the state 
of the field of theoretical material science at the time or constituted essential principles 
that a satisfactory theory of materials must uphold:
%
\begin{enumerate}[i)]
\item Theory must be closely linked to experiment.
\item Ab initio calculations without approximations are virtually impossible 
      except in model systems.
\item Approximations are frequently based on prior experience rather 
      than on theoretical justification.
\item The study of trends in the properties of homologous materials 
      is a crucial ingredient of a credible theoretical framework.
\end{enumerate}
%
These four observations provide a useful framework to analyze the current state of the field.

\section{The Rise of the Computer}
\label{sec:riseofcomp}
We shall begin, naturally, with the second of Ehrenreich's points, 
which addresses the state of ab initio calculations
and the inherent difficulties in performing them. 

The intervening thirty years has seen an enormous extension of computing capacity. 
To demonstrate this we may contrast the machines available at the time
he was writing to those that are widely available at the present time of writing (2017). 

In 1987 an example of a state of the art computer would be the Connection Machine.
\footnote{One of the employees at Thinking Machines Corporation,
the company building the connection machine, was Richard Feynman.
For an interesting discussion of the work they
did at Thinking Machines and Feynman's time there see W. Daniel
Hillis' article in Physics Today 42, 2, 78 (1989).}
%There was a strong focus on design at the company. Some of that
%work can be found at Tamiko Thiel's page}%; \url{http://www.tamikothiel.com/theory/cm_txts/di-frames.html}}
The CM-2 was launched in 1987 it was configured with up to 512 MB of RAM and 25 GB of RAID
hard disk space. It had 65K microprocessors (each with 600 bytes of memory). 
%Support for 32 bit floating point operations meant just over 
%2048 processors in practice per machine.

By contrast the HPC machine presently accessible to the author 
is more easily measured in nodes. Each node in a machine has 2x12 core Intel Broadwell processors.
Each node has 128 GB RAM and 120 GB static storage. The machine has in total 720 nodes. 
The processors themselves have up to 10 cores with clock speeds (roughly the speed the
processor can execute instructions) on the order of 3 GHz. A clock speed of 12.5 MHz 
would be representative in 1987. 
Each processor, on the modern machine, has abundant cache space of 10x 256 KiB L2 Cache, and 25 MiB of L3 cache,
this is to be compared with 600 bytes in the connection machine. 
Nowadays there could be on the order of 3 billion transistors in a single microprocessor;
in 1987 that number would be closer to 100,000. 

Listing specifications doesn't necessarily clarify how much more powerful machines have become, 
but it is striking that in terms of speed, storage, and transistor density 
there are 3-5 orders of magnitude improvements in each category. 
The stability and optimization of compilers and the ease of programmability
of the machines has also improved.

It is this constant development of computing capacity that 
has played an important component in enabling {\it ab initio} calculations. 

\subsection{Scales of Materials Modelling}
What are the scales involved in connecting atomic systems where quantum mechanics is important
to macroscopic systems? Chemists arrived at the mol as a convenient unit for describing
quantities of atoms. A mol is on the order of $10^{23}$ atoms. Setting up a single 
system of equations to describe the motion of and mutual interactions of 
this number of particles is not practical.

More realisitically a suitably macroscopic specimen might be considered to be on the 
order of 200 nm$^{3}$ of material. A sample of material of this size is of the 
same order as the wavelength of visible light and could be
resolved with an optical microscope. Assuming an atomic spacing of 1 tenth of a nanometer 
a dynamical simulation of around 8 billion atoms done using quantum mechanics 
could be considered a quantum mechanical simulation of a macroscopic system. 

To give an idea of the time required to perform a well converged quantum simulation 
250 Fe atoms required 4 hours on 96 nodes of the computer with the same 
specifications noted earlier. There are 2000 electrons in the simulation which must be allowed
to rearrange themselves so they find their energetic minimum. This calculation yields forces on every 
atom that are calculated to an accuracy greater than 0.1 eV/A (electron volts per angstrom).
This accuracy is determined by the limitations of the theory employed to perform the simulation
rather than numerical limitations.

If computer technology continues developing at the same rate it has
over the previous 30 years than within 60 years, with no developements in the numerical techniques, 
quantum mechanical simulations of systems of millions of atoms should be feasible
within a similar amount of computing time. 

If dynamical properties are of interest the atoms motion must be propagated forward. 
If a time step of a femtosecond ($(10^{-15})$s) is taken between evaluations of the 
forces generating a picosecond of data ($(10^{-12})$s) would require about 5 months 
computing time. Another 12 orders of magnitude efficiency would remain to be found to 
simulate the motion of particles on time scales more familiar to everyday experience
in the same period.

These considerations are artificial for two important reasons. The first
is I strongly suspect computational power will not continue developing at the rate
seen over the previous 30 years. 

The second is that even if computational resources did continue growing 
at an equivalent rate; simulations of this nature might be of little value.
The objective of {\it ab initio} modeling is to extract meaningful information
about trends and characteristic properties of different classes of materials. 
Massive simulations generate such a quantity of data they may obscure 
reasoning and consume resources without providing insight and general principles.

If, as Ehrenreich suggests, studying trends in homologous materials is truly a necessary component of a satisfactory
theory then these simulations would have to be repeated over and over again, for every element in a group,
for each compound in a series, etc... The ``one-off" character of numerical calculations makes them unsuitable
as the basis for an actual theory of trends in the measurable properties of materials.

There is also the observation that many of the macroscopic properties of materials do not require a quantum description.
Many properties of materials can be described by a scalar, like Young's modulus, Poisson ratios, 
or small tensors to describe the elasticity. Even some electronic properties 
can be described using a single number like the dielectric constant of a material.
All these properties of systems are made up of many tiny degrees of freedom.
The point at which quantum dynamical variables merge into macroscopic parameters 
should also be incorporated into a satisfactory theory.
%{this seems like it might require some additivity element similar to in entropy}

The development in computing hardware is in some ways the easy part to chart. 
Clock speed, transistor density, storage, and the proliferation of high 
performance computing clusters with fast interconnects have greatly extended 
the range of problems that can be treated with numerical simulations
simply by providing more horse power.

Along with advances in hardware there has been contemporaneous methodological developments.
These include compiler optimization, development of modern programming languages, 
shared libraries, object orientation, and stable open source platforms for 
code development, testing, and sharing. 

The important theoretical advances, i.e., the physical principles and
mathematical techniques that enable theoretical materials science 
are more difficult to quantify. Difficult to quantify in what exactly 
they are and what they have enabled. In the next section 
we try and identify the major pillars of materials modelling.

\section{Computational Craft}
What sort of computations might a theoretical material scientist
wish to compute? One wish list has been provided by Andersen, a materials scientist
may wish to ``compute groundstate properties such as cohesive energies, interatomic forces, 
charge transfer, and magnetic moments, and also excitation spectra described 
by the one-electron Green's function."\cite{anderson75} To obtain these quantities a
number of intermediate calculations must be performed.

The theoretical engines driving progress in numerical simulations of materials systems are
largely based on obtaining descriptions of the electronic eigenstates via mean field theories
like density functional theory or Hartree-Fock. The effective separation
of core and valence electronic interactions, and the various numerical algorithms for 
computing the eigenstates of electrons in material systems, 
and solving the coupled systems of linear equations which arise 
again and again when doing self-consistent electronic structure calculations.

As a guide to which developments are considered important we may
look at the citation metrics. The foundational papers in theoretical materials science
number their citations in the tens of thousands. Table~\ref{tab:foundation} lists some of the papers in the field which, as
their citation count suggests, have enabled subsequent research. These techniques are divided into
three categories, they are either theoretical justification for a calculation scheme, the description of
a technique that allows for more expedient and/or accurate calculations to be performed, 
or parameterizations of the exchange-correlation functional.

\begin{table}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|p{6cm}|l|p{6cm}|p{4cm}|}
\hline
Paper & Citations & Development & Category\\
\hline
Inhomogeneous Electron Gas \cite{hohenberg64} & 20,527 & Ground state energy of electron gas is universal functional of the density. & Theory \\
Self-Consistent Equations Including Exchange and Correlation Effects & 26,273 & Self-consistent set of equations for varying electron density. & Theory \\
Linear methods in band theory \cite{andersen75}& 4,822 & Describes LAPW and LMTO approach to electronic structure calculations. & Theory/Implementation \\
Norm-Conserving Pseudopotentials \cite{hamann79} & 2,156 & Nodeless eigenfunctions that match atomic eigenvalues.  & Pseudopotentials \\
Soft self-consistent pseudopotentials in a generalized eigenvalue formalism \cite{vanderbilt90} & 11,560 & Effective pseudopotentials for
first row and transition-metal systems.  & Pseudopotentials \\
Projector augmented-wave method \cite{blochl94paw} & 19,194 & Generalizes pseudopotential and LAPW methods; allows reconstruction of wavefunctions at nucleus.  & Pseudopotentials \\
Efficacious Form for Model Pseudopotentials \cite{kleinman82} & 3,709 & Reduces number of integrals which need to be calculated in
pseudopotential calculations. & Pseudopotentials \\
Special points for Brillouin-zone integrations \cite{monkhorst76} & 21,361 & Brillouin zone integration. & Numerical Integration \\
Ground State of the Electron Gas by a Stochastic Method \cite{ceperley80} & 8,855 & Calculation of the exchange-correlation energy of an electron gas. & Exchange Correlation \\
Self-interaction correction to density-functional approximations for many-electron systems \cite{perdew81} & 11,629 & Parameterization of
Ceperley-Alder data on free electron gas. & Exchange Correlation \\
Accurate and simple analytic representation of the electron-gas correlation energy \cite{perdew91} & 12,296 & Parameterization of exchange
correlation functional. & Exchange Correlation \\
Atoms, molecules, solids, and surfaces: Applications of the generalized gradient approximation for exchange and correlation \cite{perdew92}
& 11,418 & Incorporates information from the gradient of the density. & Exchange Correlation \\
New Method for Calculating the One-Particle Green's Function with Application to the Electron-Gas Problem & 2,599 & A calculation scheme for
obtaining progressively more accurate approximations to the one-electron Green's function. \cite{hedin65} & Theory\\
Electron correlation in semiconductors and insulators: Band gaps and quasiparticle energies \cite{hybertsen86} & 1,929 & Extension of
electronic structure calculations to excited states. & Theory/Implementation \\
Unified Approach for Molecular Dynamics and Density-Functional Theory \cite{car85} & 5,940 & Combined molecular dynamics and density
functional theory. & Theory/Implementation \\
High-precision sampling for Brillouin-zone integration in metals \cite{methfessel89} & 2,908 & Brillouin zone integration. & Numerical Integration Technique \\
Improved tetrahedron method for Brillouin-zone integrations \cite{blochl94} &  2,986 & Brillouin zone integration. & Numerical Integration Technique\\
A new mixing of Hartree–Fock and local density‐functional theories \cite{becke93} & 7,143 & Combining Hartree-Fock and DFT. & Exchange Correlation \\
Generalized Gradient Approximation Made Simple \cite{perdew96} & 47,029 & GGA functionals improved.& Exchange Correlation \\
\end{tabular}
}
\caption{Citations are relevant up to Nov. 2017. Citations are according to the journals in which they appear, the actual number of
citations are much higher. These selections have been chosen as representative of the important theoretical and algorithmic developments
which have enabled subsequent research. In some cases there are a number of contemporary papers which
treat the same problems but failed to "catch on", or describe techniques which differ in an incremental way to the works cited here. 
\label{tab:foundation}}
\end{table}

A criticism of Table~\ref{tab:foundation} is biased towards a "plane wave" conception of electronic structure. 
These methods are suitable for crystalline systems with periodicity and can be pushed 
to describe more disordered systems on the order, as discussed in Sec.\ref{sec:riseofcomp}, 
of 1000 atoms. These sorts of techniques are also most favoured by workers studying metallurgical
systems from first principles and materials with potential electronic applications.

To broadly categorize the workers we can say a material scientist 
is interested in crystals, alloys, ceramics, glasses, and, possibly, rubbers and elastomers. They
worry about energy differences on the order of 100 meV. Chemists are interested in molecules, 
tend to use localized basis sets and exploit Monte Carlo and related methods to obtain energy 
bounds on the order of a few meV. Physicists are interested in lasers, Ising and Hubbard models, and the 
general positions of resonances and asymptotes in very cold materials and won't be discussed further.

\section{Justification by Utility}
The observation that materials processing has evolved in the manner 
of a craft appears to have been imitated in the development of the 
theory of materials science. Much of first principles modelling is reliant on
the realization that certain approximations work well. The first is 
the separatation of degrees of freedom. That is that some electrons in a solid are
tightly bound to a nucleus and behave similarly to how they might behave in an atom.
This allows these "core electrons" to be frozen out of a calculation, via some transformation,
and only valence electrons need to be considered. This is an important realization in
chemistry as well where it has been long recognized that chemical trends are dominated by
the valence electrons of atoms. Accordingly, chemical elements are grouped together in the table 
by their valency. 

\subsection{Core Electrons and Valence Electrons}
Another successful theoretical development is pseudopotentials. Focusing on the valence electrons 
has worked for chemistry.

\subsection{Exchange and Correlation}
The second approximation which works quite well is mean field treatments of 
electron-electron interactions: the treatment of exchange and correlation. 

If we look at the work done based on parameterization of the exchange correlation functional 
we see the number of citations approaches 100,000. Naturally that figure includes significant 
double counting of citing work, on the other hand  we have not included cross citations between journals in many of the 
figures quoted, so with some lucky canceellation the number is probably representative. 

This parameterization is based on what may be considered an abstract question,
``What might happen if we start squeezing electrons into an imaginary box?".
This imaginary box has been pondered for close to a century now. 
The practical utility of these considerations is unreasonable. 
The outcome of these considerations have resulted in a family of 
unremarkable curves that describe some energy relationship between
the density of electrons at a point in space and an energy. 
The practical consequence is this curve enables the
structural, electronic, and vibrational properties of an 
enormous class of materials to be calculated with creditable accuracy.

Just how creditable this approach is returns again to point three. 
The theoretical justifications for the success of the approach are
secondary to the fact that prior experience suggests these techniques work, 
and the rate of their acceptance and adoption seems to have 
tracked their usage in a positive feedback loop. 

If you look through a typical density functional based electronic structure code it is interesting to look at the routine
responsible for calculating the local exchange correlation potential. The input required is the 
electronic density at a point in space \footnote{Beyond the local density
approximation there a range of increasingly complex parameterizations and
approximations to the exchange-correlation functional. For instance some functionals
require the gradient of the density at a point in space.} This 
density is inserted as the sole argument in a subroutine and a single number is returned. The curve
returned has little more structure than that given in Fig.~\ref{fig:ldapz}. 
%
\begin{figure}
\begin{center}
\graphicspath{{./intro/}}\input{./intro/pz.tex}
\caption{Perdew-Zunger parameterization of the correlation and exchange energies for an electron gas.
This curve and those like it enable a great deal of contemporary materials modelling. For reference
a crystal of iron would have an electronic density of around 0.36 e/a.u.$^{3}$ (electrons per cubic
atomic unit). A silicon crystal would have an electronic density in the range of 0.02 e/a.u.$^{3}$,
carbon in the diamond conformation would be around 0.1 e/a.u.$^{-3}$.  
\label{fig:ldapz}.}
\end{center}
\end{figure}
%
Curves of this type have been used to perform calculations and simulations for 
materials composed of elements from across the periodic table. They have been
used to compute all the properties of interest, cohesive energies, magnetic moments,
etc. with varying degrees of success.

Despite its apparent simplicity, in all manner of crystalline systems, 
this functional enables the theoretical
prediction of accurate ground state properties and structures entirely from first principles. 
There are trends at the moment to reparameterize the exchange-correlation functional
in order that simulations return a better description of experimental data. The extent
to which this reparameterization is helpful is debateable. 

Part of the appeal of the simplest local density approximations is that it is universal. 
Where a calculation fails to reproduce an experimental data point, and this failure is attributed to the
exchange correlation functional, it will be argued that applying a more sophisticated treatment
is preferred to introducing a complicated reparameterization of the exchange correlation
function, based on a mixture of theoretical and post hoc experimental justification.

\section{Bayesian Materials Science}
The ubiquity of access to {\it ab initio} data has had a significant impact on 
Ehrenreich's first point: experimental corroboration. 
Cursory surveys of the literature demonstrate the increasing frequency of 
appearance of publications which contain no original experimental work. 

This is partly down to the high level of specialization of contemporary 
researchers and research groups. Previously, the distinction between
an experimental materials scientist and a theoretical one was not so clear cut. 
In addition many important theoretical developments came 
from the context of attempting to explain a new set of experimental data.
%This paragraph needs research:
%Bethe's calculation of the hyperfine? shift, the recursion work on the magnetic 
%moment of $FeAl_{3}$, I think Bardeen mentioned his work on superconductivity was
%initiated when they were trying, Shockley's work on the transistor. (A good source 
%for these sorts of anecdotal history can be found in Nobel prize banquet speeches.)
Ehrenreich's four points help clarify a desirable objective for materials modelling:
a minimal theoretical model informed by {\it ab initio} calculations that accounts for 
experimental data, and {\emph predicts} quantitatively trends in material properties. 

In these notes the possibility of exploiting the invariance theorem 
and recursion techniques is assessed as the optimal framework
for achieving this. 

A metric for the quality of the approach can be defined using 
Bayesian probability theory. The Bayesian framework lets us define a metric 
that increases as the number of free parameters in the model is reduced,
and the reproduction of target experimental data is increased, 
with a bound on optimal predictions.

The posterior on the parameters of the model can be assessed:
%
\begin{equation}
\label{eq:bayes}
P(\mathbf{w}|D, \mathcal{H}_{i}) = 
\frac{P(D|\mathbf{w}, \mathcal{H}_{i})P(\mathbf{w}|\mathcal{H})}{P(D|\mathcal{H}_{i})},
\end{equation}
%
where D, the target data, should constitute experimental data. 
Eq.~\ref{eq:bayes} MacKay summarises as:
%
\begin{equation}
{\rm Posterior} = \frac{{\rm Likelihood} \times {\rm Prior}}{{\rm Evidence}}
\end{equation}
%
\begin{equation}
\label{eq:bayesH}
P(\mathcal{H}_{i}|D) \propto P(D|\mathcal{H}_{i})P(\mathcal{H}_{i})
\end{equation}
%
\begin{equation}
\label{eq:bayesH}
P(D|\mathcal{H}_{i}) = P(D|\mathbf{w}, \mathcal{H}_{i})P(\mathbf{w}|\mathcal{H}_{i})d\mathbf{w}
\end{equation}
%
Models (hypotheses) of material systems can then be ranked according to Eq.~\ref{eq:bayesH},  

\section{Prospective}
The purpose of these notes is to establish a comprehensive framework 
for incorporating knowledge about material systems into transparent models. 
These can then be used to compute the properties of materials that may interest engineers.

The general procedure is to use ab initio data to map 
to rigorous local models of the atomic environment. The local models
of atomic interaction become easier to store, physical intuition is clarified,
and the computational workload is reduced. 

Subsequent chapters will discuss the theory underlying each of these planks; a
theory of the local atomic environment, a theory of electronic excitations, long
range order in material systems, and Bayesian techniques. All of these techniques
have been developed by other workers over a number of years but collecting 
them into a single place still seems a worthwhile endeavour. Of particular interest
is providing detailed working of the sometimes tedious algebra that underlies
important results.

%In chapter \ref{chap:odlr} the passage from discrete atomic models to continuum mechanics
%where collective variables, for instance stress and strain tensors, arise is discussed. 
%Finally, in Chapter \ref{chap:bayes} the criterion for assessing the quality of {\it ab initio} 
%models is discussed which ranks models positively according to their ability to 
%describe target experimental data, and penalizes them for the number of 
%adjustable parameters they require.

Ideally a formalism will be arrived at where engineers can consult a simple table of coefficients
and directly perform the necessary computations, ideally with pen and paper, to compute within acceptable
levels of accuracy, quantum mechanical properties of material systems: i.e. band gaps, 
conductivities, mechanical properties, and superconducting transition temperatures. 

